{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classificação MultiLabel",
      "provenance": [],
      "authorship_tag": "ABX9TyNNyhFbFp6da/Jg/RCO1qxc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amadords/Projetos-Publicos/blob/master/Classifica%C3%A7%C3%A3o_MultiLabel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtj-LGpBJWQH"
      },
      "source": [
        "Classificação MultiLabel\n",
        "---\n",
        "\n",
        "[![LinkedIn](https://img.shields.io/badge/LinkedIn-DanielSousaAmador-cyan.svg)](https://www.linkedin.com/in/daniel-sousa-amador)\n",
        "[![GitHub](https://img.shields.io/badge/GitHub-amadords-darkblue.svg)](https://github.com/amadords)\n",
        "[![Medium](https://img.shields.io/badge/Medium-DanielSousaAmador-white.svg)](https://daniel-s-amador.medium.com/)\n",
        "\n",
        "![img](https://image.freepik.com/vetores-gratis/na-ilustracao-do-conceito-de-caminho_114360-1583.jpg)\n",
        "\n",
        "\n",
        "    Observação: O presente notebook é material complementar para o artigo Classificação MultiLabel que você pode ler no link abaixo e que servirá de base teórica para tudo o que será feito abaixo.\n",
        "\n",
        "[Classificação MultiLabel](link)\n",
        "\n",
        "Em uma classificação, normalmente seguimos um caminho apenas. Treinamos algum algoritmo para que ele tente nos retornar a qual classe acha que determinado dado pertence. Além disso o que ele pode nos retornar é a probabilidade de ser de determinada classe ou não, porém sempre será retornada a classe com maior probabilidade como resultado da previsão.\n",
        "Mas quando precisamos mudar esse caminho? Quando precisamos decidir além da **classificação binária** (Sobrevive ao desastre do Titanic ou Não, por exemplo) e da **Multi Classe** (Iris Versicolor, Virginica ou Setosa, por exemplo), o que fazer? Se houvesse a possibilidade de alguém sobreviver e não sobreviver ou de uma flor ser, por exemplo Versicolor e Virginica, ao mesmo tempo?\n",
        "Talvez seja difícil pensar que esse tipo de cenário existe e caso exista, que a gente possa se deparar com ele, correto?! Mas vamos pensar no seguinte cenário: Em uma classificador de músicas, no **Spotify**, por exemplo, preciso saber se determinada música é, não apenas alegre, mas se ela também é relaxante, ou se determinada música é melancólica e depressiva etc. Então dentro desse cenário, parece fazer sentido pensar em classificar músicas em mais de um modo, correto? É isso que iremos fazer agora.\n",
        "\n",
        "Note que usaremos apenas a Métrica de Avaliação **Hamming Loss** que utiliza a fórmula:\n",
        "\n",
        "$$\n",
        "\\frac{Somatório\\,dos\\,Erros}{Nº\\,de\\,Labels}\n",
        "$$\n",
        "\n",
        "Quanto mais próximo de zero, melhor o resultado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YVvYJkhDuRT"
      },
      "source": [
        "**Bibliotecas Necessárias**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGEr0xTB__3t"
      },
      "source": [
        "!pip install scikit-multilearn -q\n",
        "\n",
        "# para algoritmos adaptados usaremos KNN e Hierarquical ARAM NN\n",
        "from skmultilearn.adapt import MLkNN, MLARAM\n",
        "# para transformação de problemas usaremos Binary Relevance, Classifier Chain e Label Powerset\n",
        "from skmultilearn.problem_transform import BinaryRelevance, ClassifierChain, LabelPowerset\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import hamming_loss\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nrqw1qJPEztL"
      },
      "source": [
        "## Compreendendo os Dados\n",
        "\n",
        "Veja que temos uma espécie de **Matriz Presença**, onde o 1 representa a presença e o 0 a não presença. Ou seja a primeira linha a música tem o valor 0 para `amazed-suprised`, `quiet-still`, `sad-lonely` e `angry-aggresive` porém tem o valor 1 para `happy-pleased` e `relaxing-clam`, o que significa que ela é uma música que traz felicidade e satisfação e também é relaxante, mas não pertence aos demais grupos.\n",
        "\n",
        "Diferente de um problema onde temos para cada dado uma classe específica, aqui temos várias para cada uma, portanto não teremos apenas uma coluna para classe e sim as 6 primeiras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "pAM3P08nCNxk",
        "outputId": "bb36c6dd-efb2-4b08-953c-4aa2007284c8"
      },
      "source": [
        "df= pd.read_csv('Musica.csv', sep=\",\")\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>amazed-suprised</th>\n",
              "      <th>happy-pleased</th>\n",
              "      <th>relaxing-clam</th>\n",
              "      <th>quiet-still</th>\n",
              "      <th>sad-lonely</th>\n",
              "      <th>angry-aggresive</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_Centroid</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_Rolloff</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_Flux</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_0</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_1</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_2</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_3</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_4</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_5</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_6</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_7</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_8</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_9</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_10</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_11</th>\n",
              "      <th>Mean_Acc1298_Mean_Mem40_MFCC_12</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_Centroid</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_Rolloff</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_Flux</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_0</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_1</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_2</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_3</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_4</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_5</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_6</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_7</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_8</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_9</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_10</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_11</th>\n",
              "      <th>Mean_Acc1298_Std_Mem40_MFCC_12</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_Centroid</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_Rolloff</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_Flux</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_0</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_1</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_2</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_3</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_4</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_5</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_6</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_7</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_8</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_9</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_10</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_11</th>\n",
              "      <th>Std_Acc1298_Mean_Mem40_MFCC_12</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_Centroid</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_Rolloff</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_Flux</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_0</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_1</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_2</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_3</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_4</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_5</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_6</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_7</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_8</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_9</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_10</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_11</th>\n",
              "      <th>Std_Acc1298_Std_Mem40_MFCC_12</th>\n",
              "      <th>BH_LowPeakAmp</th>\n",
              "      <th>BH_LowPeakBPM</th>\n",
              "      <th>BH_HighPeakAmp</th>\n",
              "      <th>BH_HighPeakBPM</th>\n",
              "      <th>BHSUM1</th>\n",
              "      <th>BHSUM2</th>\n",
              "      <th>BHSUM3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.132498</td>\n",
              "      <td>0.077848</td>\n",
              "      <td>0.229227</td>\n",
              "      <td>0.602629</td>\n",
              "      <td>0.512861</td>\n",
              "      <td>0.467404</td>\n",
              "      <td>0.529733</td>\n",
              "      <td>0.573498</td>\n",
              "      <td>0.592831</td>\n",
              "      <td>0.520031</td>\n",
              "      <td>0.598853</td>\n",
              "      <td>0.537699</td>\n",
              "      <td>0.780658</td>\n",
              "      <td>0.462982</td>\n",
              "      <td>0.407108</td>\n",
              "      <td>0.684364</td>\n",
              "      <td>0.135824</td>\n",
              "      <td>0.245631</td>\n",
              "      <td>0.157515</td>\n",
              "      <td>0.301285</td>\n",
              "      <td>0.350107</td>\n",
              "      <td>0.459476</td>\n",
              "      <td>0.583274</td>\n",
              "      <td>0.430053</td>\n",
              "      <td>0.416198</td>\n",
              "      <td>0.581916</td>\n",
              "      <td>0.342758</td>\n",
              "      <td>0.309345</td>\n",
              "      <td>0.388929</td>\n",
              "      <td>0.323521</td>\n",
              "      <td>0.455207</td>\n",
              "      <td>0.261390</td>\n",
              "      <td>0.027559</td>\n",
              "      <td>0.149077</td>\n",
              "      <td>0.195433</td>\n",
              "      <td>0.571354</td>\n",
              "      <td>0.326404</td>\n",
              "      <td>0.246745</td>\n",
              "      <td>0.524645</td>\n",
              "      <td>0.354798</td>\n",
              "      <td>0.240244</td>\n",
              "      <td>0.239788</td>\n",
              "      <td>0.128689</td>\n",
              "      <td>0.173252</td>\n",
              "      <td>0.204863</td>\n",
              "      <td>0.131632</td>\n",
              "      <td>0.245653</td>\n",
              "      <td>0.144607</td>\n",
              "      <td>0.258203</td>\n",
              "      <td>0.470051</td>\n",
              "      <td>0.259909</td>\n",
              "      <td>0.613640</td>\n",
              "      <td>0.458314</td>\n",
              "      <td>0.434716</td>\n",
              "      <td>0.448941</td>\n",
              "      <td>0.370609</td>\n",
              "      <td>0.285647</td>\n",
              "      <td>0.663082</td>\n",
              "      <td>0.297080</td>\n",
              "      <td>0.273671</td>\n",
              "      <td>0.286411</td>\n",
              "      <td>0.197026</td>\n",
              "      <td>0.196244</td>\n",
              "      <td>0.164323</td>\n",
              "      <td>0.030017</td>\n",
              "      <td>0.253968</td>\n",
              "      <td>0.008473</td>\n",
              "      <td>0.240602</td>\n",
              "      <td>0.136735</td>\n",
              "      <td>0.058442</td>\n",
              "      <td>0.107594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.384281</td>\n",
              "      <td>0.355249</td>\n",
              "      <td>0.167190</td>\n",
              "      <td>0.853089</td>\n",
              "      <td>0.260577</td>\n",
              "      <td>0.332757</td>\n",
              "      <td>0.153930</td>\n",
              "      <td>0.519381</td>\n",
              "      <td>0.268043</td>\n",
              "      <td>0.251955</td>\n",
              "      <td>0.459922</td>\n",
              "      <td>0.430814</td>\n",
              "      <td>0.654323</td>\n",
              "      <td>0.641021</td>\n",
              "      <td>0.356511</td>\n",
              "      <td>0.647367</td>\n",
              "      <td>0.367659</td>\n",
              "      <td>0.539078</td>\n",
              "      <td>0.100569</td>\n",
              "      <td>0.133502</td>\n",
              "      <td>0.337194</td>\n",
              "      <td>0.319752</td>\n",
              "      <td>0.349012</td>\n",
              "      <td>0.171182</td>\n",
              "      <td>0.191357</td>\n",
              "      <td>0.390569</td>\n",
              "      <td>0.289253</td>\n",
              "      <td>0.208641</td>\n",
              "      <td>0.341328</td>\n",
              "      <td>0.265669</td>\n",
              "      <td>0.273736</td>\n",
              "      <td>0.181791</td>\n",
              "      <td>0.028513</td>\n",
              "      <td>0.252827</td>\n",
              "      <td>0.258190</td>\n",
              "      <td>0.011351</td>\n",
              "      <td>0.236247</td>\n",
              "      <td>0.069285</td>\n",
              "      <td>0.192754</td>\n",
              "      <td>0.154258</td>\n",
              "      <td>0.128671</td>\n",
              "      <td>0.116726</td>\n",
              "      <td>0.059704</td>\n",
              "      <td>0.073697</td>\n",
              "      <td>0.080341</td>\n",
              "      <td>0.062701</td>\n",
              "      <td>0.075672</td>\n",
              "      <td>0.041256</td>\n",
              "      <td>0.207782</td>\n",
              "      <td>0.300735</td>\n",
              "      <td>0.888274</td>\n",
              "      <td>0.444000</td>\n",
              "      <td>0.294673</td>\n",
              "      <td>0.210429</td>\n",
              "      <td>0.132036</td>\n",
              "      <td>0.167474</td>\n",
              "      <td>0.205996</td>\n",
              "      <td>0.155514</td>\n",
              "      <td>0.086631</td>\n",
              "      <td>0.071462</td>\n",
              "      <td>0.067492</td>\n",
              "      <td>0.093526</td>\n",
              "      <td>0.085649</td>\n",
              "      <td>0.025101</td>\n",
              "      <td>0.182955</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.156764</td>\n",
              "      <td>0.270677</td>\n",
              "      <td>0.191377</td>\n",
              "      <td>0.153728</td>\n",
              "      <td>0.197951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.541782</td>\n",
              "      <td>0.356491</td>\n",
              "      <td>0.152246</td>\n",
              "      <td>0.791142</td>\n",
              "      <td>0.228276</td>\n",
              "      <td>0.471278</td>\n",
              "      <td>0.378166</td>\n",
              "      <td>0.559905</td>\n",
              "      <td>0.279949</td>\n",
              "      <td>0.555830</td>\n",
              "      <td>0.521424</td>\n",
              "      <td>0.477018</td>\n",
              "      <td>0.541562</td>\n",
              "      <td>0.383692</td>\n",
              "      <td>0.301562</td>\n",
              "      <td>0.209283</td>\n",
              "      <td>0.469560</td>\n",
              "      <td>0.435643</td>\n",
              "      <td>0.117424</td>\n",
              "      <td>0.233075</td>\n",
              "      <td>0.214409</td>\n",
              "      <td>0.270485</td>\n",
              "      <td>0.448359</td>\n",
              "      <td>0.446847</td>\n",
              "      <td>0.272785</td>\n",
              "      <td>0.626050</td>\n",
              "      <td>0.457692</td>\n",
              "      <td>0.257535</td>\n",
              "      <td>0.486894</td>\n",
              "      <td>0.433737</td>\n",
              "      <td>0.429170</td>\n",
              "      <td>0.250437</td>\n",
              "      <td>0.173474</td>\n",
              "      <td>0.240775</td>\n",
              "      <td>0.166080</td>\n",
              "      <td>0.134165</td>\n",
              "      <td>0.149816</td>\n",
              "      <td>0.312213</td>\n",
              "      <td>0.351309</td>\n",
              "      <td>0.550984</td>\n",
              "      <td>0.241480</td>\n",
              "      <td>0.351404</td>\n",
              "      <td>0.324881</td>\n",
              "      <td>0.102732</td>\n",
              "      <td>0.214608</td>\n",
              "      <td>0.182897</td>\n",
              "      <td>0.131488</td>\n",
              "      <td>0.092507</td>\n",
              "      <td>0.539964</td>\n",
              "      <td>0.291858</td>\n",
              "      <td>0.894329</td>\n",
              "      <td>0.316151</td>\n",
              "      <td>0.345686</td>\n",
              "      <td>0.218272</td>\n",
              "      <td>0.254165</td>\n",
              "      <td>0.392987</td>\n",
              "      <td>0.243410</td>\n",
              "      <td>0.529933</td>\n",
              "      <td>0.272590</td>\n",
              "      <td>0.153845</td>\n",
              "      <td>0.314687</td>\n",
              "      <td>0.198082</td>\n",
              "      <td>0.108067</td>\n",
              "      <td>0.140574</td>\n",
              "      <td>0.099303</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.593985</td>\n",
              "      <td>0.105114</td>\n",
              "      <td>0.025555</td>\n",
              "      <td>0.122965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.174288</td>\n",
              "      <td>0.243935</td>\n",
              "      <td>0.254326</td>\n",
              "      <td>0.438987</td>\n",
              "      <td>0.480346</td>\n",
              "      <td>0.472862</td>\n",
              "      <td>0.473128</td>\n",
              "      <td>0.777076</td>\n",
              "      <td>0.376471</td>\n",
              "      <td>0.650178</td>\n",
              "      <td>0.518224</td>\n",
              "      <td>0.374862</td>\n",
              "      <td>0.717025</td>\n",
              "      <td>0.548050</td>\n",
              "      <td>0.385528</td>\n",
              "      <td>0.545977</td>\n",
              "      <td>0.266415</td>\n",
              "      <td>0.581436</td>\n",
              "      <td>0.213768</td>\n",
              "      <td>0.371697</td>\n",
              "      <td>0.267091</td>\n",
              "      <td>0.518605</td>\n",
              "      <td>0.521327</td>\n",
              "      <td>0.375596</td>\n",
              "      <td>0.516414</td>\n",
              "      <td>0.768118</td>\n",
              "      <td>0.668286</td>\n",
              "      <td>0.482510</td>\n",
              "      <td>0.515023</td>\n",
              "      <td>0.425579</td>\n",
              "      <td>0.448576</td>\n",
              "      <td>0.305121</td>\n",
              "      <td>0.095215</td>\n",
              "      <td>0.406613</td>\n",
              "      <td>0.200305</td>\n",
              "      <td>0.234013</td>\n",
              "      <td>0.173066</td>\n",
              "      <td>0.391382</td>\n",
              "      <td>0.352153</td>\n",
              "      <td>0.333506</td>\n",
              "      <td>0.491522</td>\n",
              "      <td>0.243034</td>\n",
              "      <td>0.189499</td>\n",
              "      <td>0.161029</td>\n",
              "      <td>0.185126</td>\n",
              "      <td>0.252305</td>\n",
              "      <td>0.203846</td>\n",
              "      <td>0.112363</td>\n",
              "      <td>0.440058</td>\n",
              "      <td>0.608967</td>\n",
              "      <td>0.854473</td>\n",
              "      <td>0.478910</td>\n",
              "      <td>0.378549</td>\n",
              "      <td>0.407442</td>\n",
              "      <td>0.393774</td>\n",
              "      <td>0.284080</td>\n",
              "      <td>0.622807</td>\n",
              "      <td>0.461229</td>\n",
              "      <td>0.444935</td>\n",
              "      <td>0.355792</td>\n",
              "      <td>0.298915</td>\n",
              "      <td>0.235793</td>\n",
              "      <td>0.220195</td>\n",
              "      <td>0.235834</td>\n",
              "      <td>0.024988</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.117169</td>\n",
              "      <td>0.210526</td>\n",
              "      <td>0.057288</td>\n",
              "      <td>0.134575</td>\n",
              "      <td>0.091509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.347436</td>\n",
              "      <td>0.155448</td>\n",
              "      <td>0.100047</td>\n",
              "      <td>0.126026</td>\n",
              "      <td>0.456950</td>\n",
              "      <td>0.539992</td>\n",
              "      <td>0.301537</td>\n",
              "      <td>0.598898</td>\n",
              "      <td>0.361990</td>\n",
              "      <td>0.484839</td>\n",
              "      <td>0.646532</td>\n",
              "      <td>0.520983</td>\n",
              "      <td>0.887006</td>\n",
              "      <td>0.550650</td>\n",
              "      <td>0.373552</td>\n",
              "      <td>0.373699</td>\n",
              "      <td>0.178554</td>\n",
              "      <td>0.150008</td>\n",
              "      <td>0.045284</td>\n",
              "      <td>0.227232</td>\n",
              "      <td>0.183270</td>\n",
              "      <td>0.309233</td>\n",
              "      <td>0.188338</td>\n",
              "      <td>0.302628</td>\n",
              "      <td>0.275626</td>\n",
              "      <td>0.399990</td>\n",
              "      <td>0.478468</td>\n",
              "      <td>0.478095</td>\n",
              "      <td>0.644428</td>\n",
              "      <td>0.705417</td>\n",
              "      <td>0.737404</td>\n",
              "      <td>0.418726</td>\n",
              "      <td>0.340997</td>\n",
              "      <td>0.548333</td>\n",
              "      <td>0.273571</td>\n",
              "      <td>0.535455</td>\n",
              "      <td>0.514268</td>\n",
              "      <td>0.438880</td>\n",
              "      <td>0.367572</td>\n",
              "      <td>0.526058</td>\n",
              "      <td>0.485215</td>\n",
              "      <td>0.456886</td>\n",
              "      <td>0.424624</td>\n",
              "      <td>0.335312</td>\n",
              "      <td>0.485055</td>\n",
              "      <td>0.385908</td>\n",
              "      <td>0.523032</td>\n",
              "      <td>0.340229</td>\n",
              "      <td>0.503828</td>\n",
              "      <td>0.422766</td>\n",
              "      <td>0.918634</td>\n",
              "      <td>0.686384</td>\n",
              "      <td>0.474845</td>\n",
              "      <td>0.366783</td>\n",
              "      <td>0.218394</td>\n",
              "      <td>0.370754</td>\n",
              "      <td>0.481513</td>\n",
              "      <td>0.542680</td>\n",
              "      <td>0.695774</td>\n",
              "      <td>0.783841</td>\n",
              "      <td>0.827826</td>\n",
              "      <td>0.715683</td>\n",
              "      <td>0.573359</td>\n",
              "      <td>0.412368</td>\n",
              "      <td>0.016398</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>0.081703</td>\n",
              "      <td>0.721805</td>\n",
              "      <td>0.108737</td>\n",
              "      <td>0.172882</td>\n",
              "      <td>0.189934</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   amazed-suprised  happy-pleased  relaxing-clam  ...    BHSUM1    BHSUM2    BHSUM3\n",
              "0                0              1              1  ...  0.136735  0.058442  0.107594\n",
              "1                1              0              0  ...  0.191377  0.153728  0.197951\n",
              "2                0              1              0  ...  0.105114  0.025555  0.122965\n",
              "3                0              0              1  ...  0.057288  0.134575  0.091509\n",
              "4                0              0              0  ...  0.108737  0.172882  0.189934\n",
              "\n",
              "[5 rows x 77 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_aB1e41GMYs"
      },
      "source": [
        "**Tamanho da base de dados**\n",
        "\n",
        "592 linhas x 77 colunas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4t7rc65Cbh3",
        "outputId": "d6f5b46f-523a-49ad-90d3-0c3d7491a71b"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(592, 77)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMcIn_QoGqUf"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqyFv6Q1Gt_2"
      },
      "source": [
        "**Separação da classe e previsores**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPiBMI5RCfpz",
        "outputId": "3821b325-5526-417b-906b-f546244d8e05"
      },
      "source": [
        "classe = df.iloc[:,0:6].values\n",
        "previsores = df.iloc[:,7:78].values\n",
        "classe"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 1, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 1],\n",
              "       [0, 1, 0, 0, 0, 1],\n",
              "       ...,\n",
              "       [0, 0, 1, 1, 1, 0],\n",
              "       [0, 1, 1, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS2QNMp8G7m5"
      },
      "source": [
        "**Divisão de treino e teste**\n",
        "\n",
        "Lembre que o `random_state` garante que a \"aleatoriedade\" na divisão seja sempre a mesma. Isso significa que sempre que for rodada a divisão ela será a mesma, garantindo que o resultado não seja diferente sempre que for necessária a reexecução. 12 foi o número que escolhi, mas pode ser qualquer valor que você queira."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MknAXL6FC9iH"
      },
      "source": [
        "X_treinamento, X_teste, y_treinamento, y_teste = train_test_split(previsores,classe,test_size = 0.3,\n",
        "                                                                  random_state = 12)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aRuaafPHgov"
      },
      "source": [
        "## Algoritmo Adaptado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLUWygtBHpiF"
      },
      "source": [
        "###KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_bIUq2hDDb9",
        "outputId": "1d7a6b0b-4904-4ef8-b441-05a8c3241cf4"
      },
      "source": [
        "knn = MLkNN(k=3) \n",
        "knn.fit(X_treinamento, y_treinamento) "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLkNN(ignore_first_neighbours=0, k=3, s=1.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4BjXCrPJBGe"
      },
      "source": [
        "**Previsão com dados de teste**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfNaR3WLJJea",
        "outputId": "34b04442-4db1-4c70-ba0d-0afe30be79de"
      },
      "source": [
        "previsto = knn.predict(X_teste) \n",
        "\n",
        "# atenção na saída e motivo de não ser visualizada\n",
        "previsto"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<178x6 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 314 stored elements in List of Lists format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2CsJg_oJLgx"
      },
      "source": [
        "**Hamming para avaliar preformance**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkZRZdHNJAJ2",
        "outputId": "15e7555b-4935-4d6a-fc92-21a3eb80f482"
      },
      "source": [
        "print(hamming_loss(y_teste, previsto)) "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.20880149812734083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8-HMZ1YHvVJ"
      },
      "source": [
        "### Hierarquical ARAM NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZwJRfqwDGT-",
        "outputId": "e8eb99e1-e992-4aae-be69-ff4a0e3dc1a0"
      },
      "source": [
        "aram = MLARAM()\n",
        "aram.fit(X_treinamento, y_treinamento) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLARAM(neurons=[<skmultilearn.adapt.mlaram.Neuron object at 0x7f694619beb8>,\n",
              "                <skmultilearn.adapt.mlaram.Neuron object at 0x7f694619bf28>,\n",
              "                <skmultilearn.adapt.mlaram.Neuron object at 0x7f694619bf60>,\n",
              "                <skmultilearn.adapt.mlaram.Neuron object at 0x7f694619bf98>,\n",
              "                <skmultilearn.adapt.mlaram.Neuron object at 0x7f69460ec048>,\n",
              "                <skmultilearn.adapt.mlaram.Neuron object at 0x7f6946...\n",
              "                <skmultilearn.adapt.mlaram.Neuron object at 0x7f69460ec550>,\n",
              "                <skmultilearn.adapt.mlaram.Neuron object at 0x7f69460ec588>,\n",
              "                <skmultilearn.adapt.mlaram.Neuron object at 0x7f69460ec5c0>,\n",
              "                <skmultilearn.adapt.mlaram.Neuron object at 0x7f69460ec5f8>,\n",
              "                <skmultilearn.adapt.mlaram.Neuron object at 0x7f69460ec630>,\n",
              "                <skmultilearn.adapt.mlaram.Neuron object at 0x7f69460ec668>, ...],\n",
              "       threshold=0.02, vigilance=0.9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnsOD8rGIlQa"
      },
      "source": [
        "**Previsão com dados de teste**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucryPQKCIku_",
        "outputId": "06997807-c4c6-45e0-a3eb-bba20d9bfce1"
      },
      "source": [
        "previsto = aram.predict(X_teste) \n",
        "previsto"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 1., 1.],\n",
              "       [1., 1., 0., 0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 1., 1., 0., 0., 0.],\n",
              "       [0., 1., 1., 1., 0., 0.],\n",
              "       [0., 1., 1., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efh6WWecIqKt"
      },
      "source": [
        "**Hamming para avaliar preformance**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQJTQzY4DHMG",
        "outputId": "d395c033-d74b-46f3-ed6e-f96131ea4ccc"
      },
      "source": [
        "print(hamming_loss(y_teste, previsto)) "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.21910112359550563\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjoeZWo2HlGj"
      },
      "source": [
        "## Transformação de Problema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os2sYVzqIGXM"
      },
      "source": [
        "### Binary Relevance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOGnYMYTDI9k",
        "outputId": "11dd47c1-310b-4685-9179-6e09e460feaf"
      },
      "source": [
        "binary = BinaryRelevance(classifier = SVC())\n",
        "binary.fit(X_treinamento, y_treinamento)\n",
        "previsao = binary.predict(X_teste)\n",
        "print(hamming_loss(y_teste, previsao))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.21722846441947566\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZXhDNx2IJAW"
      },
      "source": [
        "### Classifier Chain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJuyHSLWDKLW",
        "outputId": "3bab84d7-e7fd-46c4-b997-509475270b8d"
      },
      "source": [
        "chain = ClassifierChain(classifier = SVC())\n",
        "chain.fit(X_treinamento, y_treinamento)\n",
        "previsoes = chain.predict(X_teste)\n",
        "print(hamming_loss(y_teste,previsoes))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.23314606741573032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSKQpjvtILYs"
      },
      "source": [
        "### Label PowerSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xtwjjIEDLeI",
        "outputId": "ed452d54-5320-43d9-90d0-317c4b36d851"
      },
      "source": [
        "label = LabelPowerset(classifier = SVC())\n",
        "label.fit(X_treinamento, y_treinamento)\n",
        "previsoes = label.predict(X_teste)\n",
        "print(hamming_loss(y_teste,previsoes))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.19288389513108614\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-shhIdHiKHyV"
      },
      "source": [
        "# Obrigado!\n",
        "\n",
        "Obrigado por ter disponibilizado um pouco do seu tempo e atenção aqui. Espero que, de alguma forma, tenha sido útil para seu crescimento. Se houver qualquer dúvida ou sugestão, não hesite em entrar em contato no [LinkedIn](https://www.linkedin.com/in/daniel-sousa-amador) e verificar meus outros projetos no [GitHub](https://github.com/amadords).\n",
        "\n",
        "[![LinkedIn](https://img.shields.io/badge/LinkedIn-DanielSousaAmador-cyan.svg)](https://www.linkedin.com/in/daniel-sousa-amador)\n",
        "[![GitHub](https://img.shields.io/badge/GitHub-amadords-darkblue.svg)](https://github.com/amadords)\n",
        "[![Medium](https://img.shields.io/badge/Medium-DanielSousaAmador-white.svg)](https://daniel-s-amador.medium.com/)\n",
        "\n",
        "\n",
        "\n",
        "<center><img width=\"90%\" src=\"https://raw.githubusercontent.com/danielamador12/Portfolio/master/github.png\"></center>"
      ]
    }
  ]
}